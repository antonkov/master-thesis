\chapter{Декодер}

Пример ссылок на литературные источники: \cite{hall-combinatorics,kudryashov-codingtheory,finding-and-counting-given-length-cycles,counting-short-cycles-of-quasi-cyclic-protograph-ldpc-codes,message-passing-algorithm-for-counting-short-cycles-in-graph,how-to-find-long-paths-efficiently,color-coding,algorithm-for-counting-for-counting-short-cycles-in-bipartite-graphs,on-the-number-of-cycles-in-a-graph,understanding-belief-propogation,mackay-codes}.


Проведение экспериментального исследования зависимости эффективности итеративного декодирования от некоторого
критерия подразумевает непосредственное моделирование передачи информации через канал с шумом посредством 
кодирования информации определенным МППЧ-кодом и последующим декодированием с помощью итеративного декодера.

Критерий, основанный на спектре циклов графа Таннера, побуждает к проведению исследования на иррегулярных
МППЧ-кодах. Еще одним фактором не в пользу скорости декодирования является размер кода --- который
должен быть выбран в соотвествии с длинами кодов, используемыми на практике, которые достаточно велики.
Кроме того, количество переданных кодовых слов при моделировании должны быть достаточно большим, чтобы оценка
отношения ошибочно-переданных блоком к общему числу переданных блоков (FER - frame error rate) была достаточно точна.

Моделирование передачи достаточного количества кодовых слов занимает минуты на современных 
компьютерах, используя наивную реализацию декодера на CPU или реализацию из доступных библиотек 
(таких как например реализация из Communication System Toolbox в Matlab). 
С учетом того, что каждый код должен быть протестирован на десятках различных отношений сигнал-шум (SNR ---
signal noise ratio), подход с использованием наивной реализации при имеющихся вычислительных ресурсах займет
недопустимо большое количество времени. 

Альтернативным подходом являются реализации итеративных декодеров с использованием
GPU.
Многие из доступных реализаций заточены под МППЧ-коды из стандартов. Большая часть использует параллелизм
на уровне декодирования одного кодового слова, что позволяет ускорить время между получением 
битовой последовательности и декодированного кодового слова
\cite{stressing-the-ber-simulation-of-ldpc-codes-in-the-error-floor-region-using-gpu-clusters}, 
но не использует тот факт что при оценке вероятности ошибки на блок передается большое количество кодовых
слов. В данном случае значительно более существенный выигрыш позволяет получить параллелизм
на уровне набора кодовых слов, где вычислительные узлы разделены на группы, каждая из которых занимается
декодированием различных кодовых слов
\cite{opencl-cuda-algorithms-for-parallel-decoding-of-any-irregular-ldpc-code-using-gpu}.

К сожалению, итеративного GPU декодера, оптимизированного для получения оценки вероятности ошибки на блок,
не было найдено в открытых источниках сети Internet. Что побудило к созданию простого итеративного декодера
с использованием архитектуры CUDA, который позволил получить 60-70 кратное ускорение при моделировании
передачи большого числа кодовых слов иррегулярного МППЧ-кода.

\section{Количество кодовых слов при моделировании}

В первую очередь следует провести анализ количества кодовых слов, которое следует передать, чтобы 
статистически достоверно оценить вероятность ошибки на блок.

Пусть $P$ --- вероятность ошибки на блок и передача производилась до достижения $k$ ошибочно переданных блоков.
Тогда суммарно передано $N\approx \frac{k}{P}$ кодовых слов. Оценим дисперсию среднего арифметического при 
$N$ опытах. $X_i$ --- индикаторная случайная величина ошибки при передаче $i$-го кодового слова. 
\newcommand{\Expect}{\mathsf{E}}
\[
	\begin{split}
	\Variance \left({\frac{\sum_i{X_i}}{N}}\right) & =
	\Expect \left(\frac{\sum_i{X_i}}{N}\right)^2-\left(\Expect \left(\frac{\sum_i{X_i}}{N}\right)\right)^2 \\
	& = \frac{\Expect(\sum_i{X_i})^2}{N^2})-\left(\frac{\sum_i{\Expect(X_i)}}{N}\right)^2 \\
	& = \frac{\Expect(\sum_i{X_i})^2}{N^2})-\left(\frac{N \cdot P}{N}\right)^2 \\
	& = \frac{\sum_{i \neq j}{\Expect(X_i \cdot X_j)} + \sum_i{\Expect(X_i^2)}}{N^2}-P^2 \\
	& = \frac{N \cdot (N - 1) \cdot P^2 + N \cdot P}{N^2}-P^2 \\
	& = \frac{-N \cdot P^2 + N \cdot P}{N^2} \\
	& = \frac{P \cdot (1 - P)}{N}  = \frac{P^2 \cdot (1 - P)}{k}
	\end{split}
\]

Применяя Гауссовскую аппрокимацию биномиального распределения, можем говорить что пределы за $3\cdot \sigma$
маловероятны (вероятность порядка 0.001). Поэтому погрешность находится в пределах (учитывая что в реальности
$P \ll 1$):
\[
\pm \delta = 3 \cdot P \sqrt{\frac{1 - P}{k}} \approx P \cdot \frac{3}{\sqrt{k}} 
\]	 

Соответственно, например при $k=50$ получаем относительную погрешность $\approx 42\%$, 
а при $k=100$ --- $\approx 30\%$. При увеличении $k$ погрешность убывает обратно пропорционально корню.
Для проведения дальнейших исследований остановимся на $k=100$.

\section{Построение порождающей матрицы по проверочной}

Многие исследователи при моделировании передачи кодовых слов через канал останавливаются на многократной отправке
нулевого кодового слова и его последующем декодировании. Однако, при таком подходе оценка эффективности
МППЧ-кода оказывается существенно завышена относительно передачи различных кодовых слов при практическом использовании
кода.

Для генерации большого числа различных кодовых слов необходимо иметь представление кода в виде
 порождающей матрицы. Имея представление $(n,k)$ кода в виде проверочной матрицы $H$ размера $r \times n$ и
 ранга $p=n-k$, порождающую матрицу кода $G$ можно получить следующим образом:
 \begin{enumerate}
 	\item С помощью двух проходов алгоритма Гаусса преобразуем матрицу $H$ к матрице $J$ того же размера,
 	 которая выглядит как:
 	\[
 		J = \begin{pmatrix}
 			I_p & 0 \\
 			0 & 0
 		\end{pmatrix}
 	\] 
 	Действительно, с помощью первого прохода получаем матрицу с нулями ниже главной диагонали,
 	 записав линейное преобразование как матрицу $P$ размера $r \times r$:
 	\[
 		H_2 = P \cdot H = \begin{pmatrix}
 			1 & x_{1,2} & x_{1,3} & \ldots & x_{1,p} & x_{1,p+1} & \ldots & x_{1,n} \\
 			0 & 1 & x_{2,3} & \ldots & x_{2,p} & x_{2,p+1} & \ldots & x_{2,n} \\
 			\ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\
 			0 & 0 & 0 & \ldots & 1 & x_{p,p+1} & \ldots & x_{p,n} \\ 
 			0 & 0 & 0 & \ldots & 0 & 0 & \ldots & 0 \\ 
 			\ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\
 			0 & 0 & 0 & \ldots & 0 & 0 & \ldots & 0
 		\end{pmatrix}
 	\]
 	Затем с помощью еще одного прохода $H_2^T$ может быть приведен к $J^T$ записав линейное преобразование
 	в матрицу $Q$ размера $n \times n$:
 	\[
 	\begin{split}
 		J^T & = Q \cdot H_2^T \\
 		& = Q \cdot \begin{pmatrix}
 			1 & 0 & \ldots & 0 & 0 & \ldots & 0 \\
 			x_{1,2} & 1 & \ldots & 0 & 0 & \ldots & 0 \\
 			x_{1,3} & x_{2,3} & \ldots & 0 & 0 & \ldots & 0 \\
 			\ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\
 			x_{1,p} & x_{2,p} & \ldots & 1 & 0 & \ldots & 0 \\
 			x_{1,p+1} & x_{2,p+1} & \ldots & x_{p,p+1} & 0 & \ldots & 0 \\
 			\ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\
 			x_{1,n} & x_{2,n} & \ldots & x_{p,n} & 0 & \ldots & 0
 		\end{pmatrix}  = \begin{pmatrix}
 			I_p & 0 & \ldots & 0 \\
 			0 & 0 & \ldots & 0 \\
 			\ldots & \ldots & \ldots & \ldots \\
 			0 & 0 & \ldots & 0 \\
 		\end{pmatrix}
 	\end{split}
 	\]
 	Итого:
 	\[
 		J = (Q \cdot H_2^T)^T = H_2 \cdot Q^T = P \cdot H \cdot Q^T
 	\]
 	
 	\item Запишем отношение между порождающей и проверочной матрицей:
 	 \[
 	 	G \cdot H^T = 0
 	 \]
 	 или тоже самое:
 	 \[
 	 	H \cdot G^T = 0
 	 \]
 	 Выражая $H$ через $J$:
 	 \[
 	 	P^{-1} \cdot J \cdot (Q^T)^{-1} \cdot G^T = 0
 	 \]
 	 где $P^{-1}$ и $(Q^T)^{-1}$ обратные матрицы для $P$ и $Q^T$ соответственно. Обратные матрицы существуют
 	 так как проходы Гаусса сохраняли ранг матрицы, соответственно преобразование обратимо.
 	 
 	 \item Обозначим $(Q^T)^{-1} \cdot G^T$ за $Y$ ($Y$ имеет размер $n\times k$), тогда:
 	\[
 		J \cdot Y = 0
 	\]
 	откуда следует, что $Y$ имеет форму:
 	\[
 		Y=\begin{pmatrix}
 			0 & 0 & \ldots & 0 \\
 			\ldots & \ldots & \ldots & \ldots \\
 			0 & 0 & \ldots & 0 \\
 			y_{p+1,1} & y_{p+1,2} & \ldots & y_{p+1,k} \\
 			\ldots & \ldots & \ldots & \ldots \\
 			y_{n,1} & y_{n,2} & \ldots & y_{n,k} \\
 		\end{pmatrix}
 	\]
 	а
 	\[
 	G^T = Q^T \cdot Y
 	\]
 	
 	Выбирая различные коэффициенты $y_{i,j}$ в $G$ будут получатся различные наборы кодовых слов,
 	не все из которых будут являться базисами. Для получения базисного набора векторов в $G$, необходимо
 	и достаточно чтобы ранг $G$ был равен $k$, так как $det Q \neq 0$ соответственно
 	 ранг $Y$ тоже должен быть равен $k$.
 	
 	Одним из подходящих вариантов $Y$ является следующий, ранг которого равен, 
 	очевидно, $k$ (так как $n - k = p$):
 	\[
 		Y=\begin{pmatrix}
 			0 \\
 			I_k
 		\end{pmatrix}
 	\]
 	
 	Итого:
 	\[
 	G=\begin{pmatrix}
 		0 & I_k
 	\end{pmatrix} \cdot Q
 	\] 
 \end{enumerate}






































































