\chapter{Декодер}

Проведение экспериментального исследования зависимости эффективности итеративного декодирования от некоторого
критерия подразумевает непосредственное моделирование передачи информации через канал с шумом посредством 
кодирования информации определенным МППЧ-кодом и последующим декодированием с помощью итеративного декодера.

Критерий, основанный на спектре циклов графа Таннера, побуждает к проведению исследования на иррегулярных
МППЧ-кодах. Еще одним фактором не в пользу скорости декодирования является размер кода --- который
должен быть выбран в соотвествии с длинами кодов, используемыми на практике, которые достаточно велики.
Кроме того, количество переданных кодовых слов при моделировании должны быть достаточно большим, чтобы оценка
отношения ошибочно-переданных блоком к общему числу переданных блоков (FER - frame error rate) была достаточно точна.

Моделирование передачи достаточного количества кодовых слов занимает минуты на современных 
компьютерах, используя наивную реализацию декодера на CPU или реализацию из доступных библиотек 
(таких как например реализация из Communication System Toolbox в Matlab). 
С учетом того, что каждый код должен быть протестирован на десятках различных отношений сигнал-шум (SNR ---
signal noise ratio), подход с использованием наивной реализации при имеющихся вычислительных ресурсах займет
недопустимо большое количество времени. 

Альтернативным подходом являются реализации итеративных декодеров с использованием
GPU.
Многие из доступных реализаций заточены под МППЧ-коды из стандартов. Большая часть использует параллелизм
на уровне декодирования одного кодового слова, что позволяет ускорить время между получением 
битовой последовательности и декодированного кодового слова
\cite{stressing-the-ber-simulation-of-ldpc-codes-in-the-error-floor-region-using-gpu-clusters}, 
но не использует тот факт что при оценке вероятности ошибки на блок передается большое количество кодовых
слов. В данном случае значительно более существенный выигрыш позволяет получить параллелизм
на уровне набора кодовых слов, где вычислительные узлы разделены на группы, каждая из которых занимается
декодированием различных кодовых слов
\cite{opencl-cuda-algorithms-for-parallel-decoding-of-any-irregular-ldpc-code-using-gpu}.

К сожалению, итеративного GPU декодера, оптимизированного для получения оценки вероятности ошибки на блок,
не было найдено в открытых источниках сети Internet. Что побудило к созданию простого итеративного декодера
с использованием архитектуры CUDA, который позволил получить 60-70 кратное ускорение при моделировании
передачи большого числа кодовых слов иррегулярного МППЧ-кода.

\section{Количество кодовых слов при моделировании}

В первую очередь следует провести анализ количества кодовых слов, которое следует передать, чтобы 
статистически достоверно оценить вероятность ошибки на блок.

Пусть $P$ --- вероятность ошибки на блок и передача производилась до достижения $k$ ошибочно переданных блоков.
Тогда суммарно передано $N\approx \frac{k}{P}$ кодовых слов. Оценим дисперсию среднего арифметического при 
$N$ опытах. $X_i$ --- индикаторная случайная величина ошибки при передаче $i$-го кодового слова. 
\newcommand{\Expect}{\mathsf{E}}
\[
	\begin{split}
	\Variance \left({\frac{\sum_i{X_i}}{N}}\right) & =
	\Expect \left(\frac{\sum_i{X_i}}{N}\right)^2-\left(\Expect \left(\frac{\sum_i{X_i}}{N}\right)\right)^2 \\
	& = \frac{\Expect(\sum_i{X_i})^2}{N^2})-\left(\frac{\sum_i{\Expect(X_i)}}{N}\right)^2 \\
	& = \frac{\Expect(\sum_i{X_i})^2}{N^2})-\left(\frac{N \cdot P}{N}\right)^2 \\
	& = \frac{\sum_{i \neq j}{\Expect(X_i \cdot X_j)} + \sum_i{\Expect(X_i^2)}}{N^2}-P^2 \\
	& = \frac{N \cdot (N - 1) \cdot P^2 + N \cdot P}{N^2}-P^2 \\
	& = \frac{-N \cdot P^2 + N \cdot P}{N^2} \\
	& = \frac{P \cdot (1 - P)}{N}  = \frac{P^2 \cdot (1 - P)}{k}
	\end{split}
\]

Применяя Гауссовскую аппроксимацию биномиального распределения, можем говорить что пределы за $3\cdot \sigma$
маловероятны (вероятность порядка 0.001). Поэтому погрешность находится в пределах (учитывая что в реальности
$P \ll 1$):
\[
\pm \delta = 3 \cdot P \sqrt{\frac{1 - P}{k}} \approx P \cdot \frac{3}{\sqrt{k}} 
\]	 

Соответственно, например при $k=50$ получаем относительную погрешность $\approx 42\%$, 
а при $k=100$ --- $\approx 30\%$. При увеличении $k$ погрешность убывает обратно пропорционально корню.
Для проведения дальнейших исследований остановимся на $k=100$.

\section{Построение порождающей матрицы по проверочной}

Многие исследователи при моделировании передачи кодовых слов через канал останавливаются на многократной отправке
нулевого кодового слова и его последующем декодировании. Однако, при таком подходе оценка эффективности
МППЧ-кода оказывается существенно завышена относительно передачи различных кодовых слов при практическом использовании
кода.

Для генерации большого числа различных кодовых слов необходимо иметь представление кода в виде
 порождающей матрицы. Имея представление $(n,k)$ кода в виде проверочной матрицы $H$ размера $r \times n$ и
 ранга $p=n-k$, порождающую матрицу кода $G$ можно получить следующим образом:
 \begin{enumerate}
 	\item С помощью двух проходов алгоритма Гаусса преобразуем матрицу $H$ к матрице $J$ того же размера,
 	 которая выглядит как:
 	\[
 		J = \begin{pmatrix}
 			I_p & 0 \\
 			0 & 0
 		\end{pmatrix}
 	\] 
 	Действительно, с помощью первого прохода получаем матрицу с нулями ниже главной диагонали,
 	 записав линейное преобразование как матрицу $P$ размера $r \times r$:
 	\[
 		H_2 = P \cdot H = \begin{pmatrix}
 			1 & x_{1,2} & x_{1,3} & \ldots & x_{1,p} & x_{1,p+1} & \ldots & x_{1,n} \\
 			0 & 1 & x_{2,3} & \ldots & x_{2,p} & x_{2,p+1} & \ldots & x_{2,n} \\
 			\ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\
 			0 & 0 & 0 & \ldots & 1 & x_{p,p+1} & \ldots & x_{p,n} \\ 
 			0 & 0 & 0 & \ldots & 0 & 0 & \ldots & 0 \\ 
 			\ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\
 			0 & 0 & 0 & \ldots & 0 & 0 & \ldots & 0
 		\end{pmatrix}
 	\]
 	Затем с помощью еще одного прохода $H_2^T$ может быть приведен к $J^T$ записав линейное преобразование
 	в матрицу $Q$ размера $n \times n$:
 	\[
 	\begin{split}
 		J^T & = Q \cdot H_2^T \\
 		& = Q \cdot \begin{pmatrix}
 			1 & 0 & \ldots & 0 & 0 & \ldots & 0 \\
 			x_{1,2} & 1 & \ldots & 0 & 0 & \ldots & 0 \\
 			x_{1,3} & x_{2,3} & \ldots & 0 & 0 & \ldots & 0 \\
 			\ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\
 			x_{1,p} & x_{2,p} & \ldots & 1 & 0 & \ldots & 0 \\
 			x_{1,p+1} & x_{2,p+1} & \ldots & x_{p,p+1} & 0 & \ldots & 0 \\
 			\ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\
 			x_{1,n} & x_{2,n} & \ldots & x_{p,n} & 0 & \ldots & 0
 		\end{pmatrix}  = \begin{pmatrix}
 			I_p & 0 & \ldots & 0 \\
 			0 & 0 & \ldots & 0 \\
 			\ldots & \ldots & \ldots & \ldots \\
 			0 & 0 & \ldots & 0 \\
 		\end{pmatrix}
 	\end{split}
 	\]
 	Итого:
 	\[
 		J = (Q \cdot H_2^T)^T = H_2 \cdot Q^T = P \cdot H \cdot Q^T
 	\]
 	
 	\item Запишем отношение между порождающей и проверочной матрицей:
 	 \[
 	 	G \cdot H^T = 0
 	 \]
 	 или тоже самое:
 	 \[
 	 	H \cdot G^T = 0
 	 \]
 	 Выражая $H$ через $J$:
 	 \[
 	 	P^{-1} \cdot J \cdot (Q^T)^{-1} \cdot G^T = 0
 	 \]
 	 где $P^{-1}$ и $(Q^T)^{-1}$ обратные матрицы для $P$ и $Q^T$ соответственно. Обратные матрицы существуют
 	 так как проходы Гаусса сохраняли ранг матрицы, соответственно преобразование обратимо.
 	 
 	 \item Обозначим $(Q^T)^{-1} \cdot G^T$ за $Y$ ($Y$ имеет размер $n\times k$), тогда:
 	\[
 		J \cdot Y = 0
 	\]
 	откуда следует, что $Y$ имеет форму:
 	\[
 		Y=\begin{pmatrix}
 			0 & 0 & \ldots & 0 \\
 			\ldots & \ldots & \ldots & \ldots \\
 			0 & 0 & \ldots & 0 \\
 			y_{p+1,1} & y_{p+1,2} & \ldots & y_{p+1,k} \\
 			\ldots & \ldots & \ldots & \ldots \\
 			y_{n,1} & y_{n,2} & \ldots & y_{n,k} \\
 		\end{pmatrix}
 	\]
 	а
 	\[
 	G^T = Q^T \cdot Y
 	\]
 	
 	Выбирая различные коэффициенты $y_{i,j}$ в $G$ будут получатся различные наборы кодовых слов,
 	не все из которых будут являться базисами. Для получения базисного набора векторов в $G$, необходимо
 	и достаточно чтобы ранг $G$ был равен $k$, так как $\det Q \neq 0$ соответственно
 	 ранг $Y$ тоже должен быть равен $k$.
 	
 	Одним из подходящих вариантов $Y$ является следующий, ранг которого равен, 
 	очевидно, $k$ (так как $n - k = p$):
 	\[
 		Y=\begin{pmatrix}
 			0 \\
 			I_k
 		\end{pmatrix}
 	\]
 	
 	Итого:
 	\[
 	G=\begin{pmatrix}
 		0 & I_k
 	\end{pmatrix} \cdot Q
 	\] 
 \end{enumerate}

\section{Алгоритм декодирования}

В качестве алгоритма декодирования был выбран один из множества разновидностей алгоритмов декодирования
по принципу распространения доверия --- LLR-SPA (Sum-Product Algorithm in Log-Likelihood domain
--- Алгоритм суммирования произведений в терминах логарифмов).

 Алгоритм суммирования произведений входит в семейство алгоритмов передачи сообщений, 
 которые позволяют организовать исходные данные в памяти
 таким образом, чтобы можно было организовать параллельную обработку. 
 
 Переход к логарифмам был необходим в первую очередь потому, что предполагалась реализация декодера на 
 архитектуре CUDA, которая, за исключением видеокарт для научных вычислений Tesla, предоставляет только 
 возможность вычисления в типах данных с плавающей точкой одинарной точности. Соответственно переход
 к логарифмам позволил получить приемлемую точность. Кроме того использование логарифмов позволяет
 использовать более простые операции, например, сложения вместо умножения.
 
 Вкратце опишем алгоритм LLR-SPA.
 
 \subsection{LLR-SPA}
 
 Для простоты записи предположим, что мы рассматриваем $(J,K)$---регулярный МППЧ-код.
 Обозначим событие что все $J$ проверочных соотношений выполнены за $S$, а выполнение
 каждой из проверок соответственно за $S_i$.
 
  Основное предположение, которое, вообще говоря, не верно
 при наличии циклов в графе Таннера, говорит о том, что проверки независимы. То есть
 \[
 \Pr(S)=\prod_i \Pr(S_i)
 \]
 Соответственно соотношение о независимости проверок выполняется тем больше, чем ниже
 плотность единиц в проверочной матрице кода, а также зависит от многих других критериев, один из
 которых как раз рассмотрен в данной работе.

Пусть $\bs{x}$ передаваемое кодовое слово, соответственно $x_i$ --- $i$-ый символ кодового слова.
Обозначим за $\bs{y}$ принятую последовательность, за $p_i$ вероятность единицы на $i$-ом символе
принятой последовательности, основываясь на мягком входе декодера, то есть $p_i = \Pr(x_i=1|\bs{y})$.

Рассмотрим основное утверждение, которое позволит сформулировать LLR-SPA как алгоритм передачи
сообщений.

\begin{theorem}
	\cite{kudryashov-codingtheory}
	Если результаты различных проверок, в которые входит данный символ --- независимые события, а 
	$p_{jh}, h=1,\ldots,K-1, j=1,\ldots,J$---вероятность единицы для $h$-го символа $j$-й проверки при
	заданной последовательности $y$, то
	\begin{equation}
		\frac{\Pr(x_i=0|\bs{y},S)}{\Pr(x_i=1|\bs{y},S)}=\frac{1-p_i}{p_i} \cdot \prod_{j=1}^{J}
		\frac{1+\prod_{h=1,col(j,h)!=i}^{K}(1-2p_{jh})}{1-\prod_{h=1,col(j,h)!=i}^{K}(1-2p_{jh})}
	\end{equation}
	где $col(j,h)$ --- обозначает номер столбца $h$-ой единицы в $j$-ой проверке.
\end{theorem}

\begin{lemma}
	\cite{kudryashov-codingtheory}
	Пусть в последовательности из $n$ двоичных символов вероятность единицы в позиции $i$ равна $p_i$.
	Тогда вероятность четного числа единиц в последовательности равна
	\begin{equation}
		P_{even}=\frac{1+\prod_{i=1}^n(q_i-p_i)}{2}, q_i=1-p_i
	\end{equation}
\end{lemma}
\begin{proof}
	Рассмотрим бином:
	\[
		\prod_{i=1}^n(q_i-p_i)
	\]
	отрицательные слагаемые в котором соответствуют вероятностям последовательностей с нечетным
	числом единиц, а положительные с четным. Соответственно:
	\[
	P_{even} - P_{odd} = \prod_{i=1}^n(q_i-p_i)
	\]
	а
	\[
	P_{even} + P_{odd} = 1
	\]
	Таким образом складывая предыдущие выражения для получения $P_{even}$ получаем:
	\[
		P_{even}=\frac{1+\prod_{i=1}^n(q_i-p_i)}{2}, q_i=1-p_i
	\]
	\qed
\end{proof}

\begin{proof}
	По теореме Байеса:
	\begin{equation}
		\Pr(x_i=0|\bs{y},S)=\frac{\Pr(S|\bs{y},x_i=0) \cdot \Pr(x_i=0|\bs{y})}{\Pr(S|\bs{y})}
	\end{equation}
	\begin{equation}
		\Pr(x_i=1|\bs{y},S)=\frac{\Pr(S|\bs{y},x_i=1) \cdot \Pr(x_i=1|\bs{y})}{\Pr(S|\bs{y})}
	\end{equation}
	
	После подстановки:
	\begin{equation}
		\frac{\Pr(x_i=0|\bs{y},S)}{\Pr(x_i=1|\bs{y},S)}=\frac{\Pr(S|\bs{y},x_i=0) \cdot \Pr(x_i=0|\bs{y})}{\Pr(S|\bs{y},x_i=1) \cdot \Pr(x_i=1|\bs{y})}
	\end{equation}
	
	Так как $\Pr(x_i=0|\bs{y})=1-p_i$, а $\Pr(x_i=1|\bs{y})=p_i$:
	\begin{equation}
		\frac{\Pr(x_i=0|\bs{y},S)}{\Pr(x_i=1|\bs{y},S)}=\frac{1-p_i}{p_i} \cdot 
		\frac{\Pr(S|\bs{y},x_i=0)}{\Pr(S|\bs{y},x_i=1)}
	\end{equation}
	
	В первом случае:
	\begin{equation}
		\Pr(S|\bs{y},x_i=0)=\prod_{j=1}^J \Pr(S_j|\bs{y},x_i=0)
	\end{equation}
	
	Соответственно $S_j$ выполняется, когда $\sum_{k!=i}x_k$ четна, то есть:
	\[
	\Pr(S_j|\bs{y},x_i=0)=\frac{1+\prod_{h=1, col(j,h)!=i}^K(1-p_{jh}-p_{jh})}{2}
	\]
	
	Во втором случае:
	\begin{equation}
		\Pr(S|\bs{y},x_i=1)=\prod_{j=1}^J \Pr(S_j|\bs{y},x_i=1)
	\end{equation}
	
	Здесь $S_j$ выполняется, когда $\sum_{k!=i}x_k$ нечетна, то есть:
	\[
	\Pr(S_j|\bs{y},x_i=0)=\frac{1-\prod_{h=1, col(j,h)!=i}^K(1-p_{jh}-p_{jh})}{2}
	\]
	
	Собирая все вместе получаем утверждение теоремы. \qed
\end{proof}

Переформулируем формулы в терминах логарифмов. Для этого введем обозначение $\alpha$ для знака логарифма
правдоподобия и $\beta$ для абсолютной величины, со следующими индексами:
\[
	\ln \frac{1-p_i}{p_i	} = \alpha_i \beta_i
\]

\[
	\ln \frac{1-p_{jh}}{p_{jh}} = \alpha_{jh} \beta_{jh}
\]

Введем функцию $f$:
\[
	f(\beta)=\ln \frac{e^{\beta}+1}{e^{\beta}-1}, \beta>0
\]
Заметим, что
\[
	f(\beta)=- \ln \frac{e^{\beta/2}-e^{-\beta/2}}{e^{\beta/2}+e^{-\beta/2}}=-\ln \tanh(\beta/2),
\]
где
\[
	\tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}=\frac{e^{2x}-1}{e^{2x}+1}
\]

Получаем:
\[
1-2p = \frac{q-p}{q+p} = \frac{q/p - 1}{q/p + 1} = \tanh \left( \frac{\alpha \beta}{2} \right)
= \alpha e^{-f(\beta)}
\]
так как $\tanh(\alpha \beta) = \alpha \tanh(\beta)$.

Подставляя полученное выражение в основное тождество:
\[
		\alpha'_i \beta'_i= \alpha_i \beta_i + \sum_{j=1}^J 
		\ln \frac{1+\prod_{h=1,col(j,h)!=i}^{K}(1-2p_{jh})}{1-\prod_{h=1,col(j,h)!=i}^{K}(1-2p_{jh})}
\]

Из предыдущего замечания получаем
\[
\prod_{h=1,col(j,h)!=i}^{K}(1-2p_{jh}) = \prod_{h=1,col(j,h)!=i}^{K} \alpha_{jh} \cdot 
 e^{-\sum_{h=1,col(j,h)!=i}^{K} f(\beta_{jh})}
\]
Обозначим $\prod_{h=1,col(j,h)!=i}^{K} \alpha_{jh}$ за $A_{ij}$, а 
$e^{-\sum_{h=1,col(j,h)!=i}^{K}f(\beta_{jh})}$ за $T_{ij}$. Тогда:
\[
		\alpha'_i \beta'_i= \alpha_i \beta_i + \sum_{j=1}^J 
		\ln \frac{1+A_{ij}T_{ij}}{1-A_{ij}T_{ij}}
\]

Заметим некоторые полезные свойства функции $f$:
\[
	f(\beta)=\ln \frac{e^{\beta}+1}{e^{\beta}-1}=\ln \frac{1+e^{-\beta}}{1-e^{-\beta}}
=-\ln \frac{1-e^{-\beta}}{1+e^{-\beta}}, \beta>0
\]

Кроме того, если $\beta>0$, тогда $f(\beta)>0$.

Таким образом $0 < T_{ij} < 1$.

Если $A_{ij}=1$, тогда:
\[
\ln \frac{1+A_{ij}T_{ij}}{1-A_{ij}T_{ij}}=\ln \frac{1+T_{ij}}{1-T_{ij}}=f(-\ln T_{ij})
\]

Иначе если  $A_{ij}=-1$, тогда:
\[
\ln \frac{1+A_{ij}T_{ij}}{1-A_{ij}T_{ij}}=\ln \frac{1-T_{ij}}{1+T_{ij}}=-f(-\ln T_{ij})
\]

Или объединяя оба случая:
\[
\ln \frac{1+A_{ij}T_{ij}}{1-A_{ij}T_{ij}}=A_{ij} \cdot f(-\ln T_{ij})
\]

Подставляя назад получаем:
\[
		\alpha'_i \beta'_i= \alpha_i \beta_i + \sum_{j=1}^J 
		A_{ij} \cdot f(-\ln T_{ij})=
\]
\[
=	\alpha_i \beta_i + \sum_{j=1}^J \prod_{h=1,col(j,h)!=i}^{K} \alpha_{jh} \cdot
		f(\sum_{h=1,col(j,h)!=i}^{K}f(\beta_{jh}))
\]

\subsection{LLR-SPA как алгоритм передачи сообщений}

Рассмотрим некоторый граф Таннера (см. рис. \ref{dec1}).

\begin{figure}[h!]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \scalebox{0.7}{\inputTikZ{decoding1}}
  \caption{граф Таннера}
  \label{dec1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \scalebox{0.7}{\inputTikZ{decoding2}}
  \caption{Передача начальных приближений}
  \label{dec2}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
  \centering
  \scalebox{0.7}{\inputTikZ{decoding3}}
  \caption{Передача от проверочных узлов к символьным}
  \label{dec3}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \scalebox{0.7}{\inputTikZ{decoding4}}
  \caption{Передача от символьных узлов к проверочным}
  \label{dec4}
\end{subfigure}
\caption{Пример передачи сообщений в графе Таннера}
\end{figure}

\begin{enumerate}[label=\arabic*.]
\item
На первом шаге алгоритма символьные узлы передают начальные приближения о
полученных символах последовательности из канала $\alpha_i \beta_i$ проверочным узлам (см. рис. \ref{dec2}).

\pgfmathdeclarefunction{gauss}{2}{%
  \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
}

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis}[
  no markers, domain=-3:3, samples=100,
  axis lines=left, xlabel=$x$, ylabel=$Prob$,
  every axis y label/.style={at={(axis description cs:0,1)},anchor=south},
  every axis x label/.style={at=(current axis.right of origin),anchor=west},
  height=5cm, width=12cm,
  xtick={-1,1,0.5}, ytick=\empty,
  xticklabels={$-1$,$1$,$x_0$},
  enlargelimits=false, clip=false, axis on top,
  grid = major
  ]
  \addplot [very thick,cyan!50!black] {gauss(1,0.8)};
  \addplot [very thick,cyan!50!black] {gauss(-1,0.8)};

\end{axis}
\end{tikzpicture}
\caption{Распределение вероятности в канале с шумом}
\end{figure}

Для получения логарифма правдоподобия $\alpha_i\beta_i=\ln \frac{1-p}{p}$ 
по полученному значению $x_0$ из канала необходимо
провести преобразование. По договоренности единичные биты передаются значением $-1$, а нулевые значением
$0$. Шум в канале размывает вероятность в колокол нормального распределения. Соответственно
вероятность $p$ переданной единицы составляет вероятность "принадлежности" левому колоколу на картинке,
а $q=1-p$ соответственно правому.

Рассмотрим соотношение плотностей нормального распределение в произвольной точке $x_0$ и получим
формулу получения логарифма правдопадобия:
\[
\ln\frac{1-p}{p}=\ln \frac{\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x_0-1)^2}{2\sigma^2}}}
         {\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x_0+1)^2}{2\sigma^2}}} =
         \ln \frac{e^{-\frac{(x_0-1)^2}{2\sigma^2}}}
         		  {e^{-\frac{(x_0+1)^2}{2\sigma^2}}}=
         		 \ln e^{-\frac{(x_0-1)^2}{2\sigma^2}+\frac{(x_0+1)^2}{2\sigma^2}}=
\]
\[
	= \frac{-(x_0-1)^2+(x_0+1)^2}{2\sigma^2}=\frac{-x_0^2+2x_0-1+x_0^2+2x_0+1}{2\sigma^2}=
	 \frac{2x_0}{\sigma^2}
\]

\item На втором шаге каждый из проверочных узлов на основе полученной информации от символьных узлов и факта,
что сумма должна быть четно. Таким образом проверочный узел $j$ подсчитывает и отправляет новую оценку
 для символьного узла $i$ основываясь на других узлах в проверке (см. рис. \ref{dec3}):
\begin{equation} \label{step2}
Z(j,i)=\prod_{h=1,col(j,h)!=i}^{K} \alpha_{jh} \cdot
		f(\sum_{h=1,col(j,h)!=i}^{K}f(\beta_{jh}))
\end{equation}

\item На третьем шаге каждый из символьных узлов $i$ комбинирует информацию полученную от проверочных узлов,
считая что проверки независимы (что неверно в общем случае), и отсылает каждому смежному проверочному узлу $j$ 
информацию от других проверочных узлов в уравнения которых этот символ также входит (см. рис. \ref{dec4}):
\begin{equation} \label{step3}
\alpha_{ji} \beta_{ji} = L(j,i)=\alpha_i \beta_i + \sum_{h=1,row(h,i)!=j}^J Z(h,i)
\end{equation}

\item На четвертом шаге исходя из всей имеющейся информации в символьных узлах строится текущее предполагаемая
переданная последовательность:
\begin{equation} \label{step4}
	\widetilde{y}_i = \alpha_i \beta_i + \sum_{h=1}^J Z(h,i)
\end{equation}
\[
	x_i = \widetilde{y}_i < 0
\]
Если вектор $\bs{x}$ жестких решений является кодовым словом, что может быть проверено с помощью умножения
 на проверочную матрицу
$H$, алгоритм декодирования завершается успешно. В случае моделирования это может быть проверено быстрее ---
простым сравнением с переданным кодовым словом.

После чего если не достигнуто максимальное число итераций, шаги 2-4 повторяются.
 Типичное используемое число итераций, которое также использовано в данном исследовании --- 50.

Если после максимального числа итераций кодовое слово до сих пор не получено --- фиксируется ошибка при
передаче и алгоритм завершается.
\end{enumerate}
\section{Реализация алгоритма декодирования на GPU}

При переносе алгоритма на GPU используется два уровня распараллеливания вычислений. Первый уровень на
основе разделения по кодовым словам, что очень хорошо ложится на архитектуру блоков и потоков, используемую
в CUDA. Второй уровень разделения на уровне ребер --- каждая сумма по смежным ребрам из выражений
\ref{step2}, \ref{step3}, \ref{step4} при передаче сообщения
по ребру считается отдельным потоком из пула потоков.

Таким образом на первом уровне, каждый из блоков занимается декодированием одного кодового слова.
Потоки в блоке могут синхронизировать свое исполнение, что используется после окончания каждого шага
алгоритма. Также потоки в блоке могут использовать общую память. Так как при декодировании различных
кодовых слов экземпляры декодеров не взаимодействуют между собой, изолированность при работе различных
блоков является большим плюсом, позволяя не тратить время на синхронизацию. Экспериментальным путем
было решено использовать 100 блоков, таким образом параллельно декодируя 100 кодовых слов.

Для эффективного подсчета сумм из выражений \ref{step2}, \ref{step3}, \ref{step4} данные необходимо
правильно расположить в памяти для попадания в кэш, так как работа с памятью один из самых узких 
участков в алгоритме. 

Для этого продублируем информацию о ребрах, первый раз расположив ее таким образом,
чтобы ребра, соответствующие символам из одной проверки, находились последовательно, для попадания в кэш
при подсчете выражения \ref{step2}. Значения $\alpha_{jh}\beta_{jh}$ с одинаковым $j$ также должны быть
расположены последовательно. Этого можно добиться построчным обходом матрицы $H$, последовательной записью
соответствующих ребер в участок памяти и индексов границ ребер из соответствующих проверок в памяти.

Второй раз информацию о ребрах необходимо расположить так, чтобы ребра соответствующий одному символьному
узлу лежали последовательно, для быстрого вычисления выражений \ref{step3} и \ref{step4}. Кроме того,
значения $Z(h,i)$ с одинаковым $i$ также должны быть расположены последовательно. Этого можно добиться
обходом матрицы $H$ по столбцам с последовательной записью соответствующих ребер и индексов где начинаются
и заканчиваются ребра, соответствующие тому же символьному узлу в записанном блоке памяти.

Таким образом для вычисления $Z(j,i),L(j,i),\widetilde{y}_i$ из выражений \ref{step2}, \ref{step3} 
и \ref{step4} берется отдельный поток из пула потоков, который с помощью прохода по соответствующей области
памяти со смежными ребрами выполняет необходимые операции. Экспериментальным путем размер пула потоков
установлен равным $512$. Производительность оптимизировалась под видеокарту на которой производились
 эксперименты --- GeForce GTX 780 3Gb. После выполнения каждого шага производилась синхронизация потоков
 в блоке. После подсчета выражения \ref{step2} производилось округление слишком больших значений $Z(j,i)$ по
 абсолютному значение до значения $\pm 13.07$. Такая константа выбрана из тех соображений, что любое
 значение $\leq 10$ при используемых размерах кодов не дает достаточной точности, а значение $\geq 16$
 приводит к переполнению при проведении вычислений в типе данных с плавающей точкой одинарной точности.
 
 На экспериментах из исследования реализованный алгоритм позволил получить почти 60-кратное ускорения
 по сравнению с версиями на CPU. Исходный код декодера может быть найден в сети интернет
 https://github.com/antonkov/CUDA-LDPC-BER-Simulator
 

\section{Подсчет эффективности кода}

Стандартный способ оценки эффективности кода --- построения
графика вероятности ошибки на блок от отношения сигнал-шум.

При фиксированной величине отношения сигнал-шум для оценки вероятности
ошибки на блок необходимо отсылать кодовые слова до достижения некоторого порогового
значения количества ошибок на блок --- в нашем случае $k=100$, как было обсуждено ранее.
В силу того, что разработанный декодер обрабатывает несколько полученных
кодовых последовательностей одновременно, декодирование будет производиться
блоками по $D=100$ кодовых слов (наиболее оптимальное количество кодовых слов в блоке для 
одновременного декодирования установлено экспериментальным путем).
Проверка достижения порогового количества ошибок будет происходить после
обработки каждого блока.

Таким образом при фиксированном значении отношения сигнал-шум подсчет эффективности
производится следующим образом:

\begin{enumerate}
	\item Вычислить порождающую матрицу $G$ по проверочной матрице $H$.
	\item Сгенерировать достаточное количество кодовых слов для подсчета эффективности.
	Было решено генерировать $D=100$ различных кодовых слов, то есть количество равное числу слов в
	блоке. Таким образом набор кодовых слов в блоках будет одинаковым, но переданные последовательности
	будут разными за счет различных шумовых данных.
	Каждое случайно сгенерированное кодовое слово является линейной комбинацией базисных кодовых
	слов, где каждое базисное кодовое слово взято с вероятностью $\frac{1}{2}$. Генератор случайных 
	чисел инициализируется одинаковым случайным значением, соответственно подсчет эффективности
	проводится на одинаковой последовательности кодовых слов для фиксированного кода на разных
	отношениях сигнал-шум.
	\item Сгенерировать шум и прибавить шумовые данные к кодовым словам. Исходя из определения
	отношения сигнал-шум:
	\[
		SNR(dB)=10\log_{10}\left(\frac{A_{signal}^2}{A_{noise}^2}\right)
	\]
	где $A_{signal},A_{noise}$ --- среднеквадратичное значение амплитуды сигнала и шума соответственно. 
	
	Принимая амплитуды сигнала за $\pm1$ как у Галлагера считаем, что $A_{signal}=1$. Соответственно:
	\[
		A_{noise}^2 = 10^{-\frac{SNR}{10}} 
	\]
	Таким образом для моделирования шума с заданным отношением SNR необходимо сгенерировать
	данные с нормальным распределением со средним равным $0$ и дисперсией $\sigma^2=A_{noise}^2$.
	\item Производить декодирование по блокам с последующим измерением числа ошибок до достижения
	порогового значения количества ошибок.
\end{enumerate}

В данном исследовании нас будет интересовать интервал 
отношения сигнал-шум от 1 дБ до 3.5 дБ, так как большинство
случайных кодов достигают вероятности ошибки на блок $10^{-3}$
для исследуемых размеров ранее 3.5 дБ. Кроме того заметим,
что нет необходимости измерять вероятность ошибки
для кода при большем отношении
сигнал-шум, если при текущем отношении код настолько хорош,
что ошибка оказывается менее некоторой пороговой, например
$10^{-3}$. Более того это еще и очень долго,
так как вероятность ошибки при увеличении отношения сигнал-шум
может уменьшаться экспоненциально.

Шаг между отношениями сигнал шум выбран равным $0.1$,
опять же из соображений того, что вероятность может убывать
очень быстро и при большом шаге будет настолько мала, что
достижение достаточного числа ошибок займет существенное время.
Использование более малого шага нецелесообразно, так как
вероятность уменьшается не так существенно.

Иногда представление эффективности кода в виде графика зависимости
от отношения сигнал-шум неудобно, так как в таком представлении различные
коды могут быть несравнимы. Значение отношения сигнал-шум $S_0$ при котором вероятность
ошибки на блок впервые становится менее некоторого заданного значения, например $10^{-3}$,
дает гораздо более компактное представление об эффективности кода и может быть использовано
для сравнения эффективности различных кодов.

Подсчет величины $S_0$ можно организовать на основе построения графика эффективности
кода от отношения сигнал-шум до достижения необходимой вероятности, однако подсчет
может быть организован гораздо эффективнее.

Заметим, что вероятность ошибки монотонно убывает при росте отношения сигнал-шум,
соответственно можно использовать бинарный поиск по отношению сигнал-шум для нахождения
$S_0$. Однако при данном подходе подсчет вероятности при значениях больше $S_0$ может
занимать существенное время, так как мы знаем что вероятность ошибки может убывать экспоненциально.
Для экономии времени при обработке слишком больших отношении сигнал-шум ограничим количество
кодовых слов для отправки. 

Обозначим текущее отношение сигнал-шум за $S$, вероятность ошибки
при $S_0$ за $FER_{target}$ (где FER --- Frame Error Rate), полученное количество ошибок на блок
за $k$, пороговое значение количества ошибок
на блок за $k_0$, количество переданных слов за $N$. Тогда нет необходимости пересылать
более $N$ кодовых слов:
\[
	N = \frac{2 \cdot k_0}{FER_{target}}
\]

Оценим вероятность ошибочного определения положения $S$ относительно $S_0$.

Действительно, при условии что $S > S_0$ ограничение на количество переданных кодовых слов
при недостижении заданного порога количества ошибок только увеличивает вероятность правильного определения
предиката нахождения $S$ относительно $S_0$. 

При условии $S < S_0$ и передаче $N$ кодовых слов 
матожидание $k$ не менее $2 \cdot k_0$. Вспомним, что при $k_0=100$ среднеквадратичное отклонение 
составляет $\sigma=0.1$ 
и соответственно вероятность, что $k > 2 \cdot k_0 - 3 \cdot \sigma \cdot 2 \cdot k_0 = 1.4 \cdot k_0$
 более $0.999$, а вероятность, что $k > 2 \cdot k_0 - 5 \cdot \sigma \cdot 2 \cdot k_0 = k_0$ более $0.999999997$.
Таким образом вероятность, протестировав $N$ слов, получить менее $k_0$ ошибок при отношении сигнал-шум
$S < S_0$ составляет менее $3 \cdot 10^{-9}$, что достаточно мало.

В итоге для нахождения $S_0$ с помощью бинарного поиска достаточно порядка $10$ итераций при начальных
границах от 1 дБ до 5 дБ.




















































































